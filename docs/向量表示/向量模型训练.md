---
sort: 4
---


# 向量模型训练

* [算法开发手册](https://kg-nlp.github.io/Algorithm-Project-Manual/向量表示/向量模型训练.html)

* [个人知乎](https://www.zhihu.com/people/zhangyj-n)

* [domain_vector_model](https://github.com/kg-nlp/domain_vector_model)

[TOC]

参考:[向量集合](https://kg-nlp.github.io/Algorithm-Project-Manual/向量表示/向量集合.html)


> Domain-adaptive Pretraining + SimCSE + In-batch Negatives 路线

1. 基础预训练模型->领域预训练模型->领域数据无监督->标注数据获取->领域数据有监督
2. 向量模型->领域数据无监督-标注数据获取->领域数据有监督

## 说明

**Q:为什么要进行向量模型训练**

* 向量模型训练是自然语言处理（NLP）中重要的步骤之一，它具有以下几个主要的目的和优势：

  1. **语义表示**：通过向量模型训练，可以将单词、短语或句子转化为连续向量表示，即将文本数据转化为计算机可以理解和处理的形式。这些向量可以捕捉到词汇之间的语义相似性和关系，使得计算机能够更好地理解和处理文本信息。
  2. **特征提取**：基于向量模型训练的结果，可以提取出丰富的特征表示，用于各种NLP任务，如文本分类、命名实体识别、情感分析等。这些特征可以提供对文本数据更全面、更有意义的表示。
  3. **语义匹配**：向量模型训练可以用于文本的语义匹配任务，例如问答系统、搜索引擎等。通过计算向量之间的相似度或距离，可以判断两段文本之间的语义相似性，从而进行相应的匹配或推荐。
  4. **降维与可视化**：通过向量模型训练，可以将高维的文本数据映射到低维空间，以便于可视化和理解。这在处理大规模文本数据、进行数据分析和探索时非常有用。
  5. **迁移学习**：通过预训练的向量模型，可以将其应用于其他相关任务上，以提升模型性能并减少训练时间。例如，使用预训练的词向量作为初始参数，可以加速和改善特定任务的训练过程。

  总之，向量模型训练在NLP中具有重要作用，它可以帮助计算机更好地理解和处理文本信息，并为各种NLP任务提供丰富的特征表示。

**Q:向量模型训练的策略**

* 向量模型训练涉及多种策略和方法，以下是一些常用的向量模型训练策略：

  1. **基于统计的方法**：基于统计的方法包括词袋模型（Bag of Words）和TF-IDF等。词袋模型将文本表示为词汇的频率或出现次数，而TF-IDF考虑了词语在整个文档集合中的重要性。这些方法可以有效地捕捉文本的词汇特征，但忽略了词汇之间的顺序和上下文关系。
  2. **Word2Vec**：Word2Vec是一种流行的无监督学习方法，用于从大规模文本数据中训练连续词向量。它基于分布假设：出现在相似上下文中的词在语义上也是相似的。Word2Vec可以生成具有语义意义的词向量，且能够保留词汇之间的关系。
  3. **GloVe**：GloVe (Global Vectors for Word Representation) 是另一种流行的词向量训练方法。与Word2Vec不同，GloVe结合了全局上下文共现信息和局部上下文窗口信息，通过解决基于共现矩阵的线性方程组来学习词向量。
  4. **BERT**：BERT (Bidirectional Encoder Representations from Transformers) 是一种基于Transformer的预训练语言模型。BERT通过自监督训练从大规模无标签文本数据中学习通用的文本表示。它能够生成上下文敏感的词向量，并在多种NLP任务上取得了显著的性能提升。
  5. **ELMO**：ELMO (Embeddings from Language Models) 是另一种基于预训练语言模型的方法。ELMO使用双向LSTM来学习上下文相关的词向量，可以根据整个句子的语义生成不同层次的词向量表示。
  6. **Transformer-based Models**：Transformer-based Models如GPT和BERT的变种，通过预训练大规模语言模型来获得上下文敏感的词向量。这些模型在各种NLP任务上展现出了强大的性能。

  以上只是一些常见的向量模型训练策略，还有许多其他方法和技术可用于文本向量化。根据具体的任务和数据集，选择合适的策略将有助于获得更好的结果。

**Q:为什么要进行语义检索向量模型训练**

* 进行语义检索向量模型训练有以下几个主要原因：

  1. **提高搜索质量**：传统的基于关键词匹配的搜索方法可能无法准确理解用户的查询意图和文档的语义信息。语义检索向量模型可以将查询和文档转化为语义空间中的向量表示，通过计算向量之间的相似度来实现更准确的搜索结果。
  2. **解决词汇变体和同义词问题**：在搜索任务中，用户可能使用不同的词汇表达相同的意思，或者存在同义词的情况。语义检索向量模型可以将这些不同的词汇映射到相似的语义空间，并使得相关的文档能够被正确匹配。
  3. **处理复杂的查询和文档结构**：查询和文档内容可能非常复杂，包含大量的词汇和句子。语义检索向量模型能够捕捉到词汇、句法和语义的复杂结构，从而能够更全面地表示查询和文档的语义信息。
  4. **支持多语言搜索**：随着全球化的发展，多语言搜索需求越来越重要。语义检索向量模型可以学习多语言的表示，并能够实现多语言的相关性匹配，从而满足多语言搜索的需求。
  5. **提高用户体验**：通过使用语义检索向量模型，能够提供更准确、相关的搜索结果，从而提高用户的搜索体验。用户能够更快速地找到所需的信息，从而提高效率和满意度。

  综上所述，语义检索向量模型的训练可以帮助解决传统关键词匹配搜索方法的局限性，提高搜索质量和用户体验，并能够适应复杂的查询和文档结构以及多语言搜索需求。这是为什么进行语义检索向量模型训练的重要原因之一。

**Q:向量模型和语义检索向量模型的不同**

* 向量模型（Vector Model）是指将文本或语言数据表示为向量的模型。在向量模型中，每个词或句子都被转化为一个向量，从而可以进行计算、比较和匹配等操作。常见的向量模型包括词向量（Word Vectors）和句子向量（Sentence Vectors）等。

  语义检索向量模型（Semantic Retrieval Vector Model）是一种特定类型的向量模型，用于语义搜索和相关性匹配任务。语义检索向量模型将文本转化为向量表示，并通过计算向量之间的相似度来判断文本之间的语义关系。

  主要的区别如下：

  1. **应用目标**：向量模型是通用的文本表示模型，用于捕捉语言数据的表示；而语义检索向量模型专注于在语义搜索和相关性匹配任务中，通过向量之间的相似度计算来实现文本的检索。
  2. **训练方法**：向量模型可以通过不同的训练方法获得，例如基于统计的方法（如Word2Vec、GloVe）或基于深度学习的方法（如BERT、ELMo）等；而语义检索向量模型通常是在大规模语料库上进行有监督训练，采用对比损失函数来优化模型参数。
  3. **任务适应性**：向量模型可以应用于多种自然语言处理任务，如文本分类、命名实体识别等；而语义检索向量模型更专注于语义搜索和相关性匹配任务，如信息检索、相似文档推荐等。
  4. **相似度计算**：在向量模型中，相似度计算通常使用余弦相似度或欧氏距离等方法；而在语义检索向量模型中，一般通过计算向量之间的余弦相似度或其他相似度度量来判断文本之间的语义关系。

  总之，向量模型是一种通用的文本表示模型，而语义检索向量模型是一种特定类型的向量模型，专注于语义搜索和相关性匹配任务。语义检索向量模型的训练和优化方法更为特殊，针对性地解决语义匹配问题。

​	


* Q:目前市面较好的开源语义向量检索模型都有哪些,优化点有哪些

  * [向量集合](https://kg-nlp.github.io/Algorithm-Project-Manual/向量表示/向量集合.html)
  * [相关论文阅读](https://kg-nlp.github.io/Algorithm-Project-Manual/向量表示/相关论文阅读.html)
* Q:在领域数据上进行测试分析
* Q:如何训练领域向量模型

  * simcse,in batch negative 
  * 优化,跨设备负采样,难例采样

* Q:对实验结果进行总结
* Q:训练测试过程有哪些关注的点

  * 监督数据生成,prompt设计,数据过滤模糊匹配
  * 可视化部分
  * 模型转换部分

* Q:接下来计划

  * 增加数据量基于训练




### 汇总

* 领域语料 [LLM数据处理](https://kg-nlp.github.io/Algorithm-Project-Manual/大模型/LLM数据处理.html)  [建筑领域语料-20230830](百度链接)

* 领域词典  [建筑领域词典-20230830.txt](百度链接)

* 总结向量模型基础模型,训练策略,数据域,数据量,数据格式

| 向量模型             | 基础模型                                                 | 数据域        | 数据格式                 | 训练策略                                | 评测策略 |
| -------------------- | -------------------------------------------------------- | ------------- | ------------------------ | --------------------------------------- | -------- |
| Chinese Word Vectors |                                                          |               | 文本,分词列表            | 无监督SGNS/PPMI(ngram)                  |          |
| Text2vec             | ernie,lert,macbert,paraphrase-multilingual-MiniLM-L12-v2 |               | 三元组                   | 有监督ConSENT                           |          |
| uniem                | roberta                                                  | 多领域+指令集 | 句子对,三元组,带分数句对 | 有监督in-batch-negative                 |          |
| FlagEmbedding        | retromae                                                 | 悟道,simclue  | 三元组                   | retormae,in-batch-negative,跨设备负采样 |          |
| RocketQA 系列        | ernie                                                    | 多领域        | 句对,listwise            | 各种负采样,联合训练                     |          |



* **算法**
  * simcse
    * 无监督训练,使用对比学习,利用dropout机制将同一样本过两次模型得到不同的embedding
  * esimcse
    * 随机重复单词构造正例,解决句子长度敏感问题
  * diffcse
    * simcse+electra simcse进行对比学习的同时为判别器提供句向量,electra生成伪样本和RTD任务学习原始句子和伪造句子差异
  * promptbert
    * prompt+对比学习  prompt模型输出mask对应的最后一层hidden state;利用不同模板表征同一句子,然后减去模板信息,进行对比学习
  * retromae
    * 面向检索任务的预训练模型



* **其他**
  * [算法框架-文本匹配-算法汇总（持续更新）](https://zhuanlan.zhihu.com/p/465584667)
  * [代码链接-paddle](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/applications/neural_search/recall)(领域模型+无监督+有监督训练)
  * [SimCSE、ESimCSE、DiffCSE](https://blog.csdn.net/sinat_39620217/article/details/132281318?spm=1001.2014.3001.5502)
  * [DiffCSE](https://zhuanlan.zhihu.com/p/507171467)
  * [中文文本语义匹配-pytorch](https://github.com/shawroad/Semantic-Textual-Similarity-Pytorch)
  * [中文文本语义匹配2](https://github.com/Macielyoung/sentence_representation_matching)
  * [语言模型中文认知能力分析](https://github.com/twang2218/vocab-coverage)





**目前训练代码暂不开源,可以参考上述链接**

## 自监督训练

### 无监督数据获取

* 领域数据上继续微调(增量训练的数据)
* 领域数据结合通用数据微调

### 基于基础模型的无监督微调学习

### 基于向量模型的无监督微调学习

## 监督训练

### 标注数据获取

* 通过chatglm2-6b获取领域监督数据
* 结合通用数据

### 基于基础模型的监督学习

### 基于向量模型的监督学习



### 向量可视化



## 效果评估

| 策略                       | 模型                                                         | Recall@1 | Recall@5 | Recall@10 | 备注                           |
| -------------------------- | ------------------------------------------------------------ | -------- | -------- | --------- | ------------------------------ |
| -                          | m3e-small                                                    | 42.647   | 74.265   | 87.5      | 基础模型                       |
| chinese-roberta-wwm-ext    | m3e-base                                                     | 42.647   | 78.676   | 88.235    | 基础模型,图                    |
| -                          | m3e-large                                                    | 36.029   | 61.765   | 72.059    | 基础模型                       |
| -                          | bge-small-zh                                                 | 36.765   | 65.441   | 74.265    | 基础模型                       |
| -                          | bge-base-zh                                                  | 38.235   | 64.706   | 78.676    | 基础模型                       |
| -                          | bge-large-zh                                                 | 38.971   | 68.382   | 82.353    | 基础模型                       |
| chinese-macbert-base       | text2vec-base-chinese                                        | 40.441   | 72.059   | 80.147    | 基础模型,图                    |
| -                          | text2vec-large-chinese                                       | 38.235   | 77.941   | 88.971    | 基础模型                       |
| nghuyong/ernie-3.0-base-zh | text2vec-base-chinese-sentence                               | 36.029   | 69.853   | 82.353    | 基础模型                       |
| nghuyong/ernie-3.0-base-zh | text2vec-base-chinese-paraphrase                             | 40.441   | 64.706   | 78.676    | 基础模型                       |
| -                          | text2vec-bge-large-chinese                                   | 15.441   | 36.765   | 44.853    | 基础模型                       |
| -                          | Ernie-model/ernie-1.0-base-zh                                | 2.206    | 6.618    | 16.912    | 基础模型                       |
| -                          | Ernie-model/ernie-1.0-large-zh-cw                            | 6.618    | 13.971   | 18.382    | 基础模型                       |
| -                          | Ernie-model/ernie-3.0-base-zh                                | 0.0      | 0.0      | 0.735     | 基础模型,图                    |
| -                          | Ernie-model/rocketqa-zh-dureader-query-encoder               | 47.794   | 78.676   | 93.382    | 基础模型,图                    |
| -                          | Ernie-model/rocketqa-zh-base-query-encoder                   | 32.353   | 63.235   | 72.794    | 基础模型                       |
| -                          | Ernie-model/ernie-3.0-medium-zh                              | 0.735    | 5.147    | 7.353     | 基础模型,图                    |
| SimCSE                     | ernie-3.0-medium-zh                                          | 14.706   | 38.235   | 50.0      | 基础模型+无监督训练            |
| SimCSE                     | ernie-3.0-medium-600                                         | 16.176   | 41.176   | 50.0      | 领域增量模型+无监督训练        |
| SimCSE                     | rocketqa-zh-dureader-query-encoder                           | 34.559   | 68.382   | 76.471    | 基础模型+无监督训练            |
| SimCSE                     | rocketqa-zh-base-query-encoder                               | 31.618   | 62.5     | 75.0      | 基础模型+无监督训练            |
| SimCSE+in-batch negative   | ernie-3.0-medium-zh                                          | 33.088   | 66.176   | 82.353    | 基础模型+无监督+有监督训练     |
| SimCSE+in-batch negative   | ernie-3.0-medium-600                                         | 33.088   | 67.647   | 80.147    | 领域增量模型+无监督+有监督训练 |
| SimCSE+in-batch negative   | rocketqa-zh-dureader-query-encoder(rocketqa-zh-dureader-query-encoder-256) | 41.176   | 79.412   | 92.647    | 基础模型+无监督+有监督训练,图  |
| SimCSE+in-batch negative   | rocketqa-zh-base-query-encoder                               | 37.5     | 74.265   | 84.559    | 基础模型+无监督+有监督训练     |
| finetune                   | m3e-base-custom                                              | 32.353   | 68.382   | 83.824    | 微调模型↓                      |

> 

| 方法                     | 基础模型                        | Recall@1 | Recall@5 | Recall@10 | 备注              |
| ------------------------ | ------------------------------- | -------- | -------- | --------- | ----------------- |
|                          | Ernie-model/ernie-3.0-medium-zh | 0.735    | 5.147    | 7.353     |                   |
| SimCSE                   | ernie-3.0-medium-zh             | 14.706   | 38.235   | 50.0      | 无监督训练        |
| SimCSE+in-batch negative | ernie-3.0-medium-zh             | 33.088   | 66.176   | 82.353    | 无监督+有监督训练 |

