---
sort: 6
---



# 模型加速

* [算法开发手册](https://kg-nlp.github.io/Algorithm-Project-Manual)

* [个人知乎](https://zhihu.com/people/zhangyj-n)

* [model_inference]()



[TOC]



***\*文本内容梳理\****

1. 引入问题
2. 编译器优化框架,重点阅读各推理框架介绍,特点,技术栈
3. 对推理框架进行详细分析,源码,优化角度
4. 将当前实际业务中的多模型进行部署,作对比实验,给出结果

## 问题

### ***\*Q:深度学习模型推理加速的方法\****

在深度学习模型推理加速方面，有几种常见的方法：

1. 模型优化：通过对模型进行优化，可以减少推理过程中的计算量和内存消耗。一些常用的模型优化技术包括模型剪枝（pruning）、量化（quantization）、蒸馏（distillation）和模型分解（model decomposition）等。
2. 硬件加速：使用专门的硬件加速器（如GPU、TPU和FPGA等）来加速深度学习模型的推理。这些硬件加速器针对矩阵运算和并行计算进行了优化，在提供更高计算性能的同时，还可以降低功耗和延迟。
3. 并行计算：利用多个计算设备或多个计算核心进行并行计算，以提高推理速度。可以使用数据并行和模型并行等技术，将计算任务分摊到多个设备上同时进行计算。
4. 基于编译器的优化：使用编译技术对深度学习模型进行静态分析和优化，生成高效的计算图或代码。一些库和框架（如TVM、XLA和NVIDIA TensorRT）提供了基于编译器优化的推理加速功能。
5. 轻量化模型设计：通过减少模型参数和计算量，设计更轻量级的模型，以在保持较高性能的同时提高推理速度。这包括使用深度可分离卷积、设计适合移动设备的模型结构（如MobileNet、EfficientNet等）等。
6. 缓存和预处理技术：通过合理的数据缓存策略和输入数据的预处理，减少数据读取和处理的时间开销，从而提高推理速度。

需要根据具体的应用场景和需求选择合适的加速方法，有时候也需要结合多种方法进行综合优化。

### ***\*Q: tokenizer操作使用GPU还是CPU\****

对于深度学习中的分词器操作，通常使用CPU来执行比较合适。以下是一些原因：

1. 计算复杂度较低：与其他深度学习任务相比，分词器的计算复杂度较低，不需要大规模的并行计算。因此，使用CPU能够提供足够的计算能力来处理分词任务。
2. 内存需求较小：分词器通常不需要大量的存储空间，因此CPU的内存容量通常足以满足分词任务。
3. 成本效益高：使用CPU进行分词操作不需要额外的硬件设备或显卡，因此成本相对较低。
4. 并行性较低：与其他深度学习任务相比，分词操作往往没有充分的并行性，因此使用GPU并不能带来明显的加速效果。

在triton中配置的CPU计算



### ***\*Q: tensorRT和triton的区别\****

TensorRT和Triton是两个用于深度学习推理的框架，它们具有以下区别：

1. TensorRT（Tensor Real-Time）是由NVIDIA开发的用于高性能深度学习推理的优化引擎。它可以将训练好的深度学习模型优化为高效的推理引擎，以提高推理性能和减少推理延迟。TensorRT专注于在NVIDIA GPU上实现快速推理，并利用低精度计算、内存优化和并行计算等技术来提高性能。TensorRT还支持批处理推理和动态形状推理等高级功能。
2. Triton Inference Server（前身为TensorRT Inference Server）是一个开源的推理服务框架，由NVIDIA开发和维护。它提供了一个高性能的服务器环境，用于部署深度学习模型并提供推理服务。Triton支持各种硬件和推理后端，包括NVIDIA GPU、CPU和其他硬件加速器，并且可以与多种常见的深度学习框架（如TensorFlow、PyTorch和ONNX）集成。Triton还提供了灵活的模型管理功能，支持多个模型版本、模型加载和卸载，以及多种推理协议（如gRPC和HTTP）。

总结来说，TensorRT是一个针对NVIDIA GPU的优化引擎，主要用于高性能推理和低延迟的深度学习推理。而Triton是一个推理服务框架，提供了可扩展的服务器环境，支持多硬件、多推理后端，并提供灵活的模型管理和多种推理协议。使用哪个框架取决于具体的需求和应用场景。





### ***\*Q: 部署优化的学习路线(参考)\****

参考:https://www.zhihu.com/question/411393222/answer/2929848117

软件栈和硬件栈:软件栈主要针对算法，包括对深度学习模型的了解、对模型加速技术的了解;硬件栈需要了解自己选择的边缘计算设备的硬件架构，边缘计算设备的ISP、存储、算力等性能。

- 软件

- - 模型加速技术第一步分析一个模型:模型大小的衡量,指标包括***\*计算量,参数量,访存量,计算密度\****。前三个是绝对值，最后一个是相对值；访存比往往是影响推理的重要因素。不同后端平台能够通过api计算不同模型在某一设备上的指标情况。
  - 当影响运行速度的是模型的计算量时，减少模型的运算量，可以使用剪枝操作，将不要的权重删减掉。
  - 量化操作，存放量减少
  - 结构重参数化，减少访存，把网络结构里能合并的层进行合并
  - tensorrt工具实现了量化，重参数化，内存优化，多线程工具

- 硬件

- - 通用计算芯片和专用计算芯片

- 语言

- - Python是高级抽象语言，对于模型搭建训练非常方便，但部署还是需要C++





本文研究重点是***\*基于编译器的优化\****,尤其是Triton Server

- 按照官方Tutorial大致了解模型部署,资源利用率设置,优化配置,推理加速,模型集成,构建pipelines的流程
- 按照官方User Guide详细了解各个模块的原理和配置细节



## 源码分析+工程实践

- 准备动态图
- 静态图
- onnx模型

### Triton Inference Server

- 核心功能，Server端的核心功能就是模型仓库的建立和服务模式的实现,主要涉及两个仓库：

- - [***\*server仓库\****](https://link.zhihu.com/?target=https%3A//github.com/triton-inference-server/server)

  - - 该仓库介绍了如何构建TritonServer实现一个推理服务，官方推荐基于docker构建，直接从[http://nvcr.io](https://link.zhihu.com/?target=http%3A//nvcr.io)拉取镜像启动即可，考虑到driver支持版的限制，请大家选择适合自己系统的tritonserver版本。模型仓库构建好之后，很快就能启动一个docker服务，大家可以自己试试。

  - ***\*bankend仓库\****

  - - 该系列仓库是Triton模型推理的后端支持，torch/tensorflow/tensorrt/onnx这些主流的模型推理都是支持的，backend仓库里包含了这些后端的具体实现，我个人比较感兴趣的是[python_backend](https://link.zhihu.com/?target=https%3A//github.com/triton-inference-server/python_backend),用户可以基于python_backend方便的实现模型的前后处理、这样通过Triton提供的ensemble模型调度机制，在server能够快速地实现前处理、模型推理、后处理的集成，用户端就发送请求就可以了，这样可以屏蔽用户端处理带来的不确定性错误，只开放给用户发送请求的接口即可。

  - [***\*Client仓库\****](https://link.zhihu.com/?target=https%3A//github.com/triton-inference-server/client)

  - - 该仓库是用户请求的实现仓库，主要基于http/grpc提供了大量的实例，Triton将请求的实现封装成了tritonclient对象，python端可以直接通过pip安装，通过pydoc去查看tritonclient方法的功能介绍，或者通过对应的library查看功能的设计理念，我觉得基于Python实现推理请求比较快捷，而且如果前处理、推理、后处理都在server端实现，C++带来的速度收益没那么明显，反而增加了开发难度。



#### [***\*server仓库\****](https://link.zhihu.com/?target=https%3A//github.com/triton-inference-server/server)

- [Examples and Tutorials](https://github.com/triton-inference-server/server#examples-and-tutorials)

- - 基于pytorch框架,tensorflow框架的自然语言处理BERT模型

  - - [Triton部署](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling/BERT/triton) (重点)
    - [TensorRT部署](https://github.com/NVIDIA/TensorRT/tree/main/demo/BERT)

- [Build and Deploy](https://github.com/triton-inference-server/server#build-and-deploy)

- - [构建Triton server镜像环境](https://github.com/triton-inference-server/server/blob/main/docs/customization_guide/build.md#building-with-docker)(重点)

- [Using Triton](https://github.com/triton-inference-server/server#using-triton)

- - 模型准备

  - - 将一个或多个模型放入模型存储库中[model repository](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_repository.md)(重点)
    - 根据模型类型和要使用的triton功能设置配置文件[model configuration](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md)(重点)
    - 可以自定义动态库方便模型使用[Add custom operations to Triton if needed by your model](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/custom_operations.md)(有疑惑)
    - 可以流水线配置模型,自定义业务逻辑脚本 [Model Ensemble](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/architecture.md#ensemble-models) and [Business Logic Scripting (BLS)](https://github.com/triton-inference-server/python_backend#business-logic-scripting)(重点)
    - 可以优化设置模型调度策略,批处理参数以及启动的模型示例[scheduling and batching](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/architecture.md#models-and-schedulers) & [model instances](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md#instance-groups)(重点)
    - 可以使用Nvidia工具优化模型配置[Model Analyzer tool](https://github.com/triton-inference-server/model_analyzer) (重点)
    - 可以进行模型的上线,下线管理[explicitly manage what models are available by loading and unloading models](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_management.md)(重点)

  - 模型配置

  - - [Quick Start Guide](https://github.com/triton-inference-server/server/blob/main/docs/getting_started/quickstart.md)(重点)
    - Triton支持多种推理引擎,统称为后端  [backends](https://github.com/triton-inference-server/backend#where-can-i-find-all-the-backends-that-are-available-for-triton) ,包括[TensorRT](https://github.com/triton-inference-server/tensorrt_backend), [TensorFlow](https://github.com/triton-inference-server/tensorflow_backend), [PyTorch](https://github.com/triton-inference-server/pytorch_backend), [ONNX](https://github.com/triton-inference-server/onnxruntime_backend), [OpenVINO](https://github.com/triton-inference-server/openvino_backend), [Python](https://github.com/triton-inference-server/python_backend)...(重点关注onnx,paddle,python)
    - 不同的硬件设备平台支持不同的推理后端 [Backend-Platform Support Matrix](https://github.com/triton-inference-server/backend/blob/main/docs/backend_platform_support_matrix.md)(了解)
    - 使用性能分析工具和模型分析工具优化模型推理性能 [optimize performance](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/optimization.md) & [Performance Analyzer](https://github.com/triton-inference-server/client/blob/main/src/c++/perf_analyzer/README.md) and [Model Analyzer](https://github.com/triton-inference-server/model_analyzer)(重点)
    - 学习上下线模型 [manage loading and unloading models](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_management.md) (重点)
    - http/grpc请求[HTTP/REST JSON-based or gRPC protocols](https://github.com/triton-inference-server/server/blob/main/docs/customization_guide/inference_protocols.md#httprest-and-grpc-protocols)(重点)

  - 客户端请求

  - -  [Python and C++ client libraries](https://github.com/triton-inference-server/client)

- 拓展

- - 自定义容器[Customize Triton Inference Server container](https://github.com/triton-inference-server/server/blob/main/docs/customization_guide/compose.md)
  - 自定义后端[Create custom backends](https://github.com/triton-inference-server/backend) in either [C/C++](https://github.com/triton-inference-server/backend/blob/main/README.md#triton-backend-api) or [Python](https://github.com/triton-inference-server/python_backend)
  - 解耦后端和模型,为一个请求返回多个响应,或者不返回响应[decoupled backends and models](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/decoupled_models.md) 
  - 使用Triton存储库代理来添加在加载和卸载模型时运行的功能，例如身份验证、解密或转换 [Triton repository agent](https://github.com/triton-inference-server/server/blob/main/docs/customization_guide/repository_agents.md) 
  - 在Jetson和JetPack上部署Triton[Jetson and JetPack](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/jetson.md)
  - 在AWS interentia上使用Triton [Use Triton on AWS Inferentia](https://github.com/triton-inference-server/python_backend/tree/main/inferentia)

- 常见问题

- - [FAQ](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/faq.md) (重点)
  - [User Guide](https://github.com/triton-inference-server/server/blob/main/docs/README.md#user-guide)(重点重点,查看流程)



- 补充说明

- - 作为一般规则，批处理是提高GPU利用率的最有益的方法。因此，应该在模型中启用动态批处理程序。使用模型的多个实例也可以提供一些好处，但通常对于具有较小计算需求的模型最有用。大多数模型将受益于使用两个实例，但超过两个实例通常就没有用了。



***\**\*根据User Guide顺序记录下面内容\*\**\***



##### 模型存储方式[Model Repository](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_repository.md#tensorrt-models)

> 有两部分内容,Model Management和tutorial

- tritonserver启动命令

```
tritonserver --model-repository=<model-repository-path>
```

- 模型仓库目录布局

在顶级模型存储库目录中，必须有零个或多个子目录。每个子目录都包含相应模型的存储库信息。配置。PBTXT文件描述模型的模型配置.

```
  <model-repository-path>/
    <model-name>/
      [config.pbtxt]
      [<output-labels-file> ...]
      <version>/
        <model-definition-file>
      <version>/
        <model-definition-file>
      ...
    <model-name>/
      [config.pbtxt]
      [<output-labels-file> ...]
      <version>/
        <model-definition-file>
      <version>/
        <model-definition-file>
      ...
    ...
```

- 常用的几种模型,仓库设置

- - ONNX Models

```
 <model-repository-path>/
    <model-name>/
      config.pbtxt
      1/
        model.onnx


  <model-repository-path>/
    <model-name>/
      config.pbtxt
      1/
        model.onnx/
           model.onnx
           <other model files>
```

- - Python Models

```
  <model-repository-path>/
    <model-name>/
      config.pbtxt
      1/
        model.py
```

###### 模型加载卸载[Model Management](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_management.md#model-management)(需要测试时验证)

Triton提供的模型管理API是HTTP/REST和GRPC协议的一部分，也是C API的一部分。Triton以三种模式控制模式之一运行:NONE, EXPLICIT或POLL。模型控制模式决定了Triton如何处理对模型存储库的更改，以及哪些协议和api可用。

- NONE

- - Triton在启动时会试图加载模型存储库中的所有模型。Triton无法加载的模型将被标记为UNAVAILABLE，并且不能用于推理。在Triton服务运行时对模型存储库的更改将被忽略。使用模型控制协议的模型加载和卸载请求没有效果,会返回一个错误响应。在启动Triton时，通过指定——model-control-mode=none来选择模型控制模式。这是默认的模型控制模式。在Triton运行时更改模型存储库必须非常小心

  - 模型存储库中的每个模型都位于自己的子目录中。在模型子目录的内容上允许的活动取决于Triton如何使用该模型。模型的状态可以通过使用模型元数据或存储库索引api来确定。

  - - 如果模型正在加载或卸载,该子目录中的任何文件或目录都不能被添加、删除或修改
    - 如果模型从未加载过或已完全卸载,可以删除整个模型子目录，或者添加、删除或修改其中的任何内容。
    - Triton在加载模型时会使用后端共享库，因此删除或修改这些库可能会导致Triton崩溃。如果你需要更新模型的后端，必须首先完全卸载模型，修改后端共享库，然后重新加载模型。在一些操作系统上，也可以将现有的共享库移动到模型存储库之外的其他位置，复制新的共享库，然后重新加载模型。
    - 当在`config.pbtxt`中修改了模型实例的配置（比如增加或减少实例数量），并且使用的是`EXPLICIT`或`POLL`模式（这是Triton的两种模型控制模式），Triton将会更新模型实例而不是重新加载模型。这是因为Triton被设计成能在运行时动态地调整模型的配置，这样可以提高服务的可用性和效率。如果希望在修改配置后重新加载模型，你可以使用`LOAD`命令强制重新加载模型,文本编辑器在修改`config.pbtxt`文件时会在模型目录下创建一个交换文件（swap file），这可能会导致模型意外地完全重新加载.
    - 如果序列模型更新(减少实例数)，Triton将等待，直到运行中的序列完成(或超时)，然后删除序列后面的实例。

- EXPLICIT

- - 如果在使用模型控制协议加载和卸载模型时看到一些内存增长,这可能不是一个实际的内存泄漏，而是一些系统的malloc动态存储分配 启发式，导致内存无法立即释放回操作系统。为了提高内存性能，可以考虑在运行Triton时通过设置LD_PRELOAD环境变量从malloc切换到tcmalloc或jemalloc，如下所示:

```
# Using tcmalloc
LD_PRELOAD=/usr/lib/$(uname -m)-linux-gnu/libtcmalloc.so.4:${LD_PRELOAD} tritonserver --model-repository=/models ...

# Using jemalloc
LD_PRELOAD=/usr/lib/$(uname -m)-linux-gnu/libjemalloc.so:${LD_PRELOAD} tritonserver --model-repository=/models ...
```

- - Triton给定的容器环境已经安装好内存分配优化的工具包,如果本地想使用该功能安装如下包

```
# Install tcmalloc
apt-get install gperf libgoogle-perftools-dev

# Install jemalloc
apt-get install libjemalloc-dev
```

- POLL

- - 对模型存储库的更改将被检测到，Triton将根据这些更改尝试加载和卸载模型。当尝试重新加载已加载的模型时，如果由于任何原因重新加载失败，则已加载的模型将保持不变并保持加载状态。如果重新加载成功，新加载的模型将替换已经加载的模型，而不会损失模型的可用性。

  - 对模型存储库的更改可能不会立即检测到，因为Triton会定期轮询存储库。可以使用——repository-poll-secs选项控制轮询间隔。可以使用控制台日志或模型准备协议或模型控制协议的索引操作来确定模型存储库更改何时生效。

  - 在Triton轮询模型存储库和对存储库进行任何更改之间没有同步。因此，Triton可以观察到导致意外行为的部分和不完全变化。因此，不建议在生产环境中使用POLL模式。

  - 在Triton服务运行时对模型存储库的更改将被忽略。使用模型控制协议的模型加载和卸载请求没有效果,会返回一个错误响应

  - 在POLL模式下，Triton响应以下模型存储库更改:

  - - 添加和删除版本：通过添加和删除相应的版本子目录，你可以在Triton中添加或删除模型版本。例如，如果你的模型版本是1.0，你应该在Triton的模型存储区域创建一个名为1.0的子目录，并在其中包含你的模型文件（通常是一个名为model.pkl的文件）。当你想要删除一个版本时，只需要删除相应的子目录。
    - 在飞行请求的处理：Triton被设计成允许正在处理的请求在模型版本被移除后继续完成。这意味着，即使你从服务器中移除了一个版本，正在进行的请求仍然可以使用该版本完成处理。然而，新的请求将不能使用已删除的版本。
    - 默认模型版本的改变：根据你的模型版本策略，更改可用的版本可能会改变默认服务的模型版本。例如，如果你有三个版本（1.0，1.1，1.2），并且1.1是默认版本，然后你删除了1.1，那么1.2可能会成为新的默认版本，因为它是剩余版本中的最新版本



###### 模型配置(文件查看)[model config protobuf definition file](https://github.com/triton-inference-server/common/blob/main/protobuf/model_config.proto)

- DataType

```
syntax = "proto3";
package inference;

enum DataType {
  TYPE_INVALID = 0;
  TYPE_BOOL = 1;
  TYPE_UINT8 = 2;
  TYPE_UINT16 = 3;
  TYPE_UINT32 = 4;
  TYPE_UINT64 = 5;
  TYPE_INT8 = 6;
  TYPE_INT16 = 7;
  TYPE_INT32 = 8;
  TYPE_INT64 = 9;
  TYPE_FP16 = 10;
  TYPE_FP32 = 11;
  TYPE_FP64 = 12;
  TYPE_STRING = 13;
  TYPE_BF16 = 14;
}
```

- ModelRateLimiter

```
message ModelRateLimiter
{
    message Resource
    {
        string name = 1;
        bool global = 2;  // 资源是如何在这些设备或实例之间分配的。全局资源意味着这些资源可以在所有设备之间共享，而每个设备都有其自己的特定资源分配。
        uint32 count = 3; // 模型实例数量 
      }
 repeated Resource resources = 1;  // ?
 uint32 priority = 2;  // 调度优先级,优先级为2的实例将获得优先级为1的实例组的一半调度机会数。默认优先级为1。优先级为0的值将被视为优先级1。
}
```

- ModelInstanceGroup

```
message ModelInstanceGroup
{
    enum Kind {  
         KIND_AUTO = 0;  // 实例运行在GPU上或CPU上
         KIND_GPU = 1;  // 必须运行在GPU上
         KIND_CPU = 2;  // 必须运行在CPU上
         KIND_MODEL = 3;  // 根据模型或是推理后端的配置决定使用什么资源,Triton的推理服务不会覆盖上面的设置
    }

    message SecondaryDevice
    {
        enum SecondaryDeviceKind {
          KIND_NVDLA = 0;  // 目前只有TensorRT后端支持KIND_NVDLA  An NVDLA core. http://nvdla.org
        }
        SecondaryDeviceKind kind = 1;  // The secondary device kind.
        int64 device_id = 2;  // Identifier for the secondary device.
    }
    
    string name = 1;  // 实例组的可选名称。如果未指定，则名称将按<模型名称>_<组编号>的形式生成。单个实例的名称将通过唯一的实例编号和GPU索引进一步生成
     Kind kind = 4;  // 此实例组的类型。默认是KIND_AUTO。如果为KIND_AUTO或KIND_GPU，则'count'和'gpu'均有效，且可以指定。如果为KIND_CPU或KIND_MODEL，则只有'count'有效，且不能指定'gpu'。
    int32 count = 2;  // 对于分配给GPU的实例组，为'gpus'中列出的每个GPU创建的实例数量。对于分配给CPU的实例组，创建的实例数量。默认值为1。
    ModelRateLimiter rate_limiter = 6;  //  与该实例组关联的速率限制器设置。可选项，如果未指定，则不会对此实例组应用速率限制。
    repeated int32 gpus = 3; // 实例可用的GPU。对于列出的每个GPU，都可用'count'个模型实例。将'gpus'设置为空（或根本不指定）等同于列出所有可用的GPU。
    repeated SecondaryDevice secondary_devices = 8;  // 此实例组指定的实例所需的其他设备。可选。
    repeated string profile = 5;  // 对于包含多个优化配置文件的TensorRT模型，此参数指定了可用于此实例组的优化配置文件集合。推断服务器将根据输入张量的形状选择最佳的配置文件。此字段应当位于0和<PlanModel中的优化配置文件总数> - 1之间，并且仅针对TensorRT后端指定，否则将会生成错误。如果未指定，服务器将默认选择第一个优化配置文件。
    bool passive = 7;  // 这个实例组内的实例是否接受来自调度器的推断请求。如果为true，则实例将不会添加到调度器中。默认值为false。
    string host_policy = 9;  // 要与实例关联的主机策略名称。默认值设置为反映实例的设备类型，例如，KIND_CPU为"cpu"，KIND_MODEL为"model"，KIND_GPU为"gpu_<gpu_id>"。
}
```

- ModelTensorReshape 

```
message ModelTensorReshape
{
  // The shape to use for reshaping.
  repeated int64 shape = 1;
}
```

- ModelInput

```
message ModelInput
{
  enum Format {
    // The input has no specific format. This is the default.
    FORMAT_NONE = 0;

    // HWC图像格式。如果模型不支持批处理（max_batch_size = 0），则需要使用3维张量的这种格式；如果模型支持批处理（max_batch_size >= 1），则需要使用4维张量的这种格式。在这两种情况下，下面的'dims'都应该只指定3个非批处理的维度（即HWC或CHW）。
    FORMAT_NHWC = 1;

    //  CHW图像格式。如果模型不支持批处理（max_batch_size = 0），则需要使用3维张量的这种格式；如果模型支持批处理（max_batch_size >= 1），则需要使用4维张量的这种格式。在这两种情况下，下面的'dims'都应该只指定3个非批处理的维度（即HWC或CHW）。
    FORMAT_NCHW = 2;
  }

  //@@     The name of the input.
  string name = 1;

  //@@     The data-type of the input.
  DataType data_type = 2;

  //@@     The format of the input. Optional.
  Format format = 3;

  //@@     The dimensions/shape of the input tensor that must be provided
  //@@     when invoking the inference API for this model.
  repeated int64 dims = 4;

  //@@ 后端期望的此输入的形状。在传给后端之前，将对该输入进行reshape以使其符合该形状。reshape必须拥有与'dims'指定的输入形状相同数量的元素。可选。
  ModelTensorReshape reshape = 5;

  //@@  输入是否是模型的形状张量。此字段目前仅支持TensorRT模型。如果此规范不符合基础模型，将生成错误。
  bool is_shape_tensor = 6;

  //@@   是否允许输入在动态创建的批次中“不规则”。默认值为false，表示只有在两个请求中该张量具有相同形状时，它们才会被批次化。True表示即使每个请求中该张量具有不同的形状，也可以将两个请求批次化。
  bool allow_ragged_batch = 7;

  //@@  输入对于模型执行是否是可选的。如果为true，则在推断请求中不需要输入。默认值为false。
  bool optional = 8;
}
```

- ModelOutput

```
message ModelOutput
{
  //@@     The name of the output.
  string name = 1;

  //@@     The data-type of the output.
  DataType data_type = 2;

  //@@     The dimensions/shape of the output tensor.
  repeated int64 dims = 3;

  //@@     The shape produced for this output by the backend. The output will
  //@@     be reshaped from this to the shape specified in 'dims' before being
  //@@     returned in the inference response. The reshape must have the same
  //@@     number of elements as the output shape specified by 'dims'. Optional.
  ModelTensorReshape reshape = 5;

  //@@   与此输出关联的标签文件。仅对于表示分类的输出需要指定。可选。
  string label_filename = 4;

  //@@     Whether or not the output is a shape tensor to the model. This field
  //@@     is currently supported only for the TensorRT model. An error will be
  //@@     generated if this specification does not comply with underlying
  //@@     model.
  bool is_shape_tensor = 6;
}
```

- BatchInput

```
message BatchInput  // (?批量输入的参数有疑惑)
{

  enum Kind {
    //@@    'source_input'的元素数量将作为输入添加，其形状为[1]
    BATCH_ELEMENT_COUNT = 0;

    //@@     'source_input'的累积元素数量将作为输入添加，其形状为[1]。例如，如果有一批两个请求，每个请求有2个元素，则值为2的输入将添加到第一个请求中，值为4的输入将添加到第二个请求中。
    BATCH_ACCUMULATED_ELEMENT_COUNT = 1;

    //@@      除了批次中的第一个请求之外，'source_input'的累积元素数量将作为输入添加，其形状为[1]。对于批次中的第一个请求，输入的形状将为[2]，其中第一个元素的值为0。
    BATCH_ACCUMULATED_ELEMENT_COUNT_WITH_ZERO = 2;

    //@@  在批次的请求中，'source_input'的最大元素数量将作为输入添加，其形状为[max_element_count]，用于批次中的第一个请求。对于其他请求，此类输入的形状将为[0]。张量的数据将未初始化。
    BATCH_MAX_ELEMENT_COUNT_AS_SHAPE = 3;

    //@@ 在批次中的请求中，'source_input'的形状将作为形状为[batch_size, len(input_dim)]的输入添加。例如，如果将形状为[3, 1]的batch-2输入和形状为[2, 2]的batch-1输入分批处理，则批量输入的形状将为[3, 2]，值为[[3, 1], [3, 1], [2, 2]]。
    BATCH_ITEM_SHAPE = 4;

    //@@  在批次中的请求中，'source_input'的形状将作为具有单维形状[batch_size * len(input_dim)]的输入添加。例如，如果将形状为[3, 1]的batch-2输入和形状为[2, 2]的batch-1输入分批处理，则批量输入的形状将为[6]，值为[3, 1, 3, 1, 2, 2]。     
    BATCH_ITEM_SHAPE_FLATTEN = 5;
  }

  //@@       The kind of this batch input.
  Kind kind = 1;

  //@@       The name of the model inputs that the backend will create
  //@@       for this batch input.
  repeated string target_name = 2;

  //@@       The input's datatype. The data type can be TYPE_INT32 or
  //@@       TYPE_FP32.
  DataType data_type = 3;

  //@@ 后端从其中一个或多个其他输入派生每个批次输入的值。'source_input'给出这些输入的名称。
  repeated string source_input = 4;
}
```

- BatchOutput

```
message BatchOutput
{
  //@@     The kind of the batch output.
  enum Kind {

    //@@  输出应根据'source_input'的形状进行分散。输出的动态维度将设置为输入中相同维度的值。
    BATCH_SCATTER_WITH_INPUT_SHAPE = 0;
  }

  //@@ 此批次输出规范产生的输出名称。
  repeated string target_name = 1;

  //@@     The kind of this batch output.
  Kind kind = 2;

  //@@  后端从其中一个或多个输入派生每个批次输出。'source_input'给出这些输入的名称。
  repeated string source_input = 3;
}
```

- ModelVersionPolicy
- ModelOptimizationPolicy

```
message ModelOptimizationPolicy
{
  //@@ 对tf的graphdef和savedmodel以及onnx模型进行图优化,对于TensorFlow，它可以启用或禁用XLA;对于Onnx，默认情况下会启用所有优化，而-1只启用基本优化，+1只启用基本和扩展优化
  //@@
  message Graph
  {
    //@@       The optimization level. Defaults to 0 (zero) if not specified.
    //@@
    //@@         - -1: Disabled
    //@@         -  0: Framework default
    //@@         -  1+: Enable optimization level (greater values indicate
    //@@            higher optimization levels)
    //@@
    int32 level = 1;
  }
  //@@    模型调度执行优先级,当前配置只支持TensorRT models.
  enum ModelPriority {
    PRIORITY_DEFAULT = 0;
    //@@       The maximum model priority.
    PRIORITY_MAX = 1;
    //@@       The minimum model priority.
    PRIORITY_MIN = 2;
  }
  //@@     CUDA-specific optimization settings.
  message Cuda
  {
    //@@   Specification of the CUDA graph to be captured.
    //@@
    message GraphSpec
    {
      //@@         Specification of tensor dimension.
      message Shape
      {
        //@@           The dimension.
        repeated int64 dim = 1;
      }

      message LowerBound
      {
        //@@         The batch size of the CUDA graph. If 'max_batch_size' is 0,
        //@@         'batch_size' must be set to 0. Otherwise, 'batch_size' must
        //@@         be set to value between 1 and 'max_batch_size'.
        int32 batch_size = 1;

        //@@         The specification of the inputs. 'Shape' is the shape of
        //@@         the input without batching dimension.
        map<string, Shape> input = 2;
      }
      //@@         The batch size of the CUDA graph. If 'max_batch_size' is 0,
      //@@         'batch_size' must be set to 0. Otherwise, 'batch_size' must
      //@@         be set to value between 1 and 'max_batch_size'.
      int32 batch_size = 1;
      //@@         The specification of the inputs. 'Shape' is the shape of the
      //@@         input without batching dimension.
      map<string, Shape> input = 2;

      //@@      .. cpp:var:: LowerBound graph_lower_bound
      //@@
      //@@         Specify the lower bound of the CUDA graph. Optional.
      //@@         If specified, the graph can be used for input shapes and
      //@@         batch sizes that are in closed interval between the lower
      //@@         bound specification and graph specification. For dynamic
      //@@         shape model, this allows CUDA graphs to be launched
      //@@         frequently without capturing all possible shape combinations.
      //@@         However, using graph for shape combinations different from
      //@@         the one used for capturing introduces uninitialized data for
      //@@         execution and it may distort the inference result if
      //@@         the model is sensitive to uninitialized data.
      //@@
      LowerBound graph_lower_bound = 3;
    }

    //@@       Use CUDA graphs API to capture model operations and execute
    //@@       them more efficiently. Default value is false.
    //@@       Currently only recognized by TensorRT backend.
    //@@
    bool graphs = 1;

    //@@       Use busy-waiting to synchronize CUDA events to achieve minimum
    //@@       latency from event complete to host thread to be notified, with
    //@@       the cost of high CPU load. Default value is false.
    //@@       Currently only recognized by TensorRT backend.
    //@@
    bool busy_wait_events = 2;

    //@@    .. cpp:var:: GraphSpec graph_spec (repeated)
    //@@
    //@@       Specification of the CUDA graph to be captured. If not specified
    //@@       and 'graphs' is true, the default CUDA graphs will be captured
    //@@       based on model settings.
    //@@       Currently only recognized by TensorRT backend.
    //@@
    repeated GraphSpec graph_spec = 3;

    //@@    .. cpp:var:: bool output_copy_stream
    //@@
    //@@       Uses a CUDA stream separate from the inference stream to copy the
    //@@       output to host. However, be aware that setting this option to
    //@@       true will lead to an increase in the memory consumption of the
    //@@       model as Triton will allocate twice as much GPU memory for its
    //@@       I/O tensor buffers. Default value is false.
    //@@       Currently only recognized by TensorRT backend.
    //@@
    bool output_copy_stream = 4;
  }

  //@@
  //@@  .. cpp:var:: message ExecutionAccelerators
  //@@
  //@@     Specify the preferred execution accelerators to be used to execute
  //@@     the model. Currently only recognized by ONNX Runtime backend and
  //@@     TensorFlow backend.
  //@@
  //@@     For ONNX Runtime backend, it will deploy the model with the execution
  //@@     accelerators by priority, the priority is determined based on the
  //@@     order that they are set, i.e. the provider at the front has highest
  //@@     priority. Overall, the priority will be in the following order:
  //@@         <gpu_execution_accelerator> (if instance is on GPU)
  //@@         CUDA Execution Provider     (if instance is on GPU)
  //@@         <cpu_execution_accelerator>
  //@@         Default CPU Execution Provider
  //@@
  message ExecutionAccelerators
  {

    //@@     Specify the accelerator to be used to execute the model.
    //@@     Accelerator with the same name may accept different parameters
    //@@     depending on the backends.
    message Accelerator
    {
      //@@       The name of the execution accelerator.
      string name = 1;
      //@@       Additional parameters used to configure the accelerator.
      map<string, string> parameters = 2;
    }

    //@@    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
    //@@
    //@@       The preferred execution provider to be used if the model instance
    //@@       is deployed on GPU.
    //@@
    //@@       For ONNX Runtime backend, possible value is "tensorrt" as name,
    //@@       and no parameters are required.
    //@@
    //@@       For TensorFlow backend, possible values are "tensorrt",
    //@@       "auto_mixed_precision", "gpu_io".
    //@@
    //@@       For "tensorrt", the following parameters can be specified:
    //@@         "precision_mode": The precision used for optimization.
    //@@         Allowed values are "FP32" and "FP16". Default value is "FP32".
   
    //@@         "max_cached_engines": The maximum number of cached TensorRT
    //@@         engines in dynamic TensorRT ops. Default value is 100.
 
    //@@         "minimum_segment_size": The smallest model subgraph that will
    //@@         be considered for optimization by TensorRT. Default value is 3.
   
    //@@         "max_workspace_size_bytes": The maximum GPU memory the model
    //@@         can use temporarily during execution. Default value is 1GB.
    //@@
    //@@       For "auto_mixed_precision", no parameters are required. If set,
    //@@       the model will try to use FP16 for better performance.
    //@@       This optimization can not be set with "tensorrt".
    //@@
    //@@       For "gpu_io", no parameters are required. If set, the model will
    //@@       be executed using TensorFlow Callable API to set input and output
    //@@       tensors in GPU memory if possible, which can reduce data transfer
    //@@       overhead if the model is used in ensemble. However, the Callable
    //@@       object will be created on model creation and it will request all
    //@@       outputs for every model execution, which may impact the
    //@@       performance if a request does not require all outputs. This
    //@@       optimization will only take affect if the model instance is
    //@@       created with KIND_GPU.
    //@@
    repeated Accelerator gpu_execution_accelerator = 1;

    //@@    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
    //@@
    //@@       The preferred execution provider to be used if the model instance
    //@@       is deployed on CPU.
    //@@
    //@@       For ONNX Runtime backend, possible value is "openvino" as name,
    //@@       and no parameters are required.
    //@@
    repeated Accelerator cpu_execution_accelerator = 2;
  }

  //@@
  //@@  .. cpp:var:: message PinnedMemoryBuffer
  //@@
  //@@     Specify whether to use a pinned memory buffer when transferring data
  //@@     between non-pinned system memory and GPU memory. Using a pinned
  //@@     memory buffer for system from/to GPU transfers will typically provide
  //@@     increased performance. For example, in the common use case where the
  //@@     request provides inputs and delivers outputs via non-pinned system
  //@@     memory, if the model instance accepts GPU IOs, the inputs will be
  //@@     processed by two copies: from non-pinned system memory to pinned
  //@@     memory, and from pinned memory to GPU memory. Similarly, pinned
  //@@     memory will be used for delivering the outputs.
  //@@
  message PinnedMemoryBuffer
  {
    //@@       Use pinned memory buffer. Default is true.
    //@@
    bool enable = 1;
  }
  //@@     The graph optimization setting for the model. Optional.
  Graph graph = 1;
  //@@     The priority setting for the model. Optional.
  ModelPriority priority = 2;
  //@@     CUDA-specific optimization settings. Optional.
  Cuda cuda = 3;
  //@@     The accelerators used for the model. Optional.
  ExecutionAccelerators execution_accelerators = 4;
  //@@     Use pinned memory buffer when the data transfer for inputs
  //@@     is between GPU memory and non-pinned system memory.
  //@@     Default is true.
  PinnedMemoryBuffer input_pinned_memory = 5;
  //@@     Use pinned memory buffer when the data transfer for outputs
  //@@     is between GPU memory and non-pinned system memory.
  //@@     Default is true.
  PinnedMemoryBuffer output_pinned_memory = 6;

  //@@  .. cpp:var:: uint32 gather_kernel_buffer_threshold
  //@@
  //@@     The backend may use a gather kernel to gather input data if the
  //@@     device has direct access to the source buffer and the destination
  //@@     buffer. In such case, the gather kernel will be used only if the
  //@@     number of buffers to be gathered is greater or equal to
  //@@     the specified value. If 0, the gather kernel will be disabled.
  //@@     Default value is 0.
  //@@     Currently only recognized by TensorRT backend.
  //@@
  uint32 gather_kernel_buffer_threshold = 7;

  //@@  .. cpp:var:: bool eager_batching
  //@@
  //@@     Start preparing the next batch before the model instance is ready
  //@@     for the next inference. This option can be used to overlap the
  //@@     batch preparation with model execution, with the trade-off that
  //@@     the next batch might be smaller than what it could have been.
  //@@     Default value is false.
  //@@     Currently only recognized by TensorRT backend.
  //@@
  bool eager_batching = 8;
}
```

- ModelQueuePolicy
- ModelEnsembling

```
message ModelEnsembling
{
  //@@  每个步骤指定了集成中包含的模型，将集成张量名称映射到模型的输入张量，并将模型的输出张量映射到集成的张量名称
  message Step
  {
    //@@     The name of the model to execute for this step of the ensemble.
    string model_name = 1;
    //@@     The version of the model to use for inference. If -1
    //@@     the latest/most-recent version of the model is used.
    int64 model_version = 2;
    
    map<string, string> input_map = 3;
    map<string, string> output_map = 4;

    //@@     [RESERVED] currently this field is reserved for internal use, users
    //@@     must not set any value to this field to avoid unexpected behavior.
    string model_namespace = 5;
  }

  //@@     The models and the input / output mappings used within the ensemble.
  repeated Step step = 1;
}
```

- ModelParameter
- ModelWarmup
- ModelOperations
- ModelTransactionPolicy
- ModelRepositoryAgents
- ModelResponseCache
- ModelConfig



###### 教程1-模型部署[Tutorials-Part1-model-deployment](https://github.com/triton-inference-server/tutorials/tree/main/Conceptual_Guide/Part_1-model_deployment#setting-up-the-model-repository)

> 如何管理多个模型(不同框架)。

> 模型版本如何控制、如何加载和卸载模型。

> 配置文件如何修改

- 示例给出了tf&torch转onnx的模型
- 关于模型配置的参数类型说明,参考[`model_config`](https://github.com/triton-inference-server/common/blob/main/protobuf/model_config.proto)[ protobuf definition](https://github.com/triton-inference-server/common/blob/main/protobuf/model_config.proto).
- 在大多数情况下，model configuration可以省略输入和输出部分，让Triton直接从模型文件中提取这些信息。在这里，我们包括它们是为了清晰起见，因为稍后在客户端应用程序中我们需要知道输出张量的名称。

##### [模型配置(文档查看)](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md)

- 必须的配置信息

- - platform,max_batch_size,input,output
  - 对于不支持批处理的模型,,max_batch_size必须设置为0
  - 对于输入和输出

```
[Only for Inputs]当输入不是张量字典时，配置文件中的输入名称应与模型定义中前向函数输入参数的名称相同。比如 if the forward function for the Torchscript model was defined as forward(self, input0, input1), the first and second inputs should be named "input0" and "input1" respectively.

<name>__<index>: Where <name> can be any string and <index> is an integer index that refers to the position of the corresponding input/output.
This means that if there are two inputs and two outputs, the first and second inputs can be named "INPUT__0" and "INPUT__1" and the first and second outputs can be named "OUTPUT__0" and "OUTPUT__1" respectively.

如果命名不按约定,那在配置文件中要强制约束输入输出的顺序

如果Triton在推断请求中接收的输入形状与模型期望的输入形状不匹配，必须使用reshape属性。同样，如果模型生成的输出形状与Triton在响应推断请求中返回的形状不匹配，必须使用reshape属性。

模型输入可以指定allow_ragged_batch来表示输入是一个不规则的输入。该字段与动态批量处理器一起使用，以允许批处理而不强制所有请求中的输入具有相同的形状。
```



- 模型版本设置

```
version_policy: { all: {}}  所有模型都可以加载推理
version_policy: { latest: { num_versions: 2}}  指定版本
version_policy: { specific: { versions: [1,3]}}  指定范围版本
```

- 实例组设置

- - place two execution instances of the model to be available on each system GPU

```
instance_group [
    {
      count: 2
      kind: KIND_GPU
    }
  ]
```

- - place one execution instance on GPU 0 and two execution instances on GPUs 1 and 2

```
instance_group [
    {
      count: 1
      kind: KIND_GPU
      gpus: [ 0 ]
    },
    {
      count: 2
      kind: KIND_GPU
      gpus: [ 1, 2 ]
    }
]
```

> If no `count` is specified for a KIND_CPU instance group, then the default instance count will be 2 for selected backends (Tensorflow and Onnxruntime). All other backends will default to 1.

- - 速率限制

```
实例组可选地指定速率限制器配置，该配置控制速率限制器如何在组中的实例上操作。如果未启用速率限制，则忽略速率限制器配置。如果启用了速率限制，并且实例组未提供此配置，则属于该组的模型实例上的执行将不会受到速率限制器的任何限制。

执行模型实例所需的一组资源。'名称'字段标识资源，'计数'字段是指组中模型实例运行所需资源的副本数量。'全局'字段指定资源是在每个设备上还是全局共享。加载的模型不能指定与全局和非全局具有相同名称的资源。如果没有提供任何资源，则Triton假定执行模型实例不需要任何资源，并将在模型实例可用时立即开始执行。

优先级作为权重值，用于对所有模型的所有实例进行优先级排序。优先级为2的实例将获得优先级为1的实例的一半调度机会。
```

以下示例指定组中的实例需要四个“R1”和两个“R2”资源才能执行。“R2”资源是一个全局资源。此外，instance_group的速率限制器优先级为2。

```
instance_group [
    {
      count: 1
      kind: KIND_GPU
      gpus: [ 0, 1, 2 ]
      rate_limiter {
        resources [
          {
            name: "R1"
            count: 4
          },
          {
            name: "R2"
            global: True
            count: 2
          }
        ]
        priority: 2
      }
    }
  ]
```

上述配置创建了3个模型实例，每个设备上都有一个（0、1和2）。这三个实例不会相互竞争“R1”，因为“R1”对于它们各自所在的设备是本地的，但是它们将竞争“R2”，因为已指定“R2”为全局资源，这意味着“R2”在整个系统中共享。虽然这些实例不会相互竞争“R1”，但它们将与其他要求包含“R1”的模型实例竞争“R1”，这些实例在它们所在的设备上运行。



- 后端优化设置
- 批量设置,分批调度设置
- 速率限制
- 模型热加载
- 推理请求和响应缓存



###### [速率限制Rate Limiter](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/rate_limiter.md#resources)

速率限制器（Rate limiter）用于管理 Triton 中模型实例的请求调度速率。速率限制器适用于 Triton 中加载的所有模型，以便进行跨模型的优先级排序。

在没有启用速率限制（--rate-limit=off）的情况下，Triton 会在模型实例可用时立即调度请求的执行（或使用动态批处理时的一组请求）。这种行为通常适用于性能最佳化。然而，在某些情况下，同时运行所有模型可能会对服务器造成过大负载。例如，某些框架上的模型执行会动态分配内存。同时运行所有这些模型可能导致系统内存不足。

速率限制器允许推迟在某些模型实例上执行推理，以使并非所有模型都同时运行。模型的优先级用于决定下一个要调度的模型实例。

要启用速率限制，用户在启动tritonserver时必须设置--rate-limit选项。

资源由唯一名称和表示资源副本数量的计数进行标识。默认情况下，模型实例不使用速率限制器资源。通过列出资源和数量，模型实例在允许执行之前，需要在模型实例设备上指定可用数量的资源。当执行时，指定数量的资源分配给模型实例，并在执行结束后释放。可用的资源副本数量默认为列出该资源的所有模型实例中的最大值。例如，假设已加载三个模型实例 A、B 和 C，每个模型实例针对单个设备指定以下资源需求：

```
A: [R1: 4, R2: 4]
B: [R2: 5, R3: 10, R4: 5]
C: [R1: 1, R3: 7, R4: 2]
```

triton server需要的资源

```
R1: 4
R2: 5
R3: 10
R4: 5
这些值确保所有模型实例都能够成功安排。可以使用命令行中的--rate-limit-resource选项明确指定资源，从而覆盖默认值
```

默认情况下，可用的资源副本是针对每个设备的，并且模型实例的资源需求是针对与模型实例运行的设备相关联的相应资源进行强制执行的。

For tritonserver, running on a two device machine, invoked with `--rate-limit-resource=R1:10 --rate-limit-resource=R2:5:0 --rate-limit-resource=R2:8:1 --rate-limit-resource=R3:2` , available resource copies are:  (疑问为什么R3是全局设置)(?)

```
GLOBAL   => [R3: 2]
DEVICE 0 => [R1: 10, R2: 5]
DEVICE 1 => [R1: 10, R2: 8]
```



###### 集成模型[Ensemble Models](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/architecture.md#ensemble-models)

集成模型表示一个或多个模型的流水线以及这些模型之间输入和输出张量的连接。集成模型旨在用于封装涉及多个模型的程序，例如“数据预处理->推理->数据后处理”。为此使用集成模型可以避免传输中间张量的开销并最大限度地减少必须发送到Triton的请求数量。

对于集成模型，必须使用集成调度器，而不管集成中使用的模型的调度器是什么。相对于集成调度器，集成模型不是实际的模型。相反，它在模型配置中作为ModelEnsembling::Step条目指定了集成中模型之间的数据流。调度器根据规范收集每个步骤的输出张量，并将它们作为输入张量提供给其他步骤。尽管如此，从外部视图来看，集成模型仍然被视为单个模型。

集成模型将继承所涉及的模型的特点，因此请求头中的元数据必须符合集成中的模型。例如，如果其中一个模型是有状态模型，则针对集成模型的推断请求应该包含有状态模型中提到的信息，这些信息将由调度器提供给有状态模型。下面是个图像分类和分割的示例:

注意 ensemble的输入 name 是下面集成模型第一步的输入value,输出的name是集成模型最后 一步的输出value;集成模型中中间步骤的输入输出是上一步的输出value值

```
name: "ensemble_model"
platform: "ensemble"
max_batch_size: 1
input [
  {
    name: "IMAGE"
    data_type: TYPE_STRING
    dims: [ 1 ]
  }
]
output [
  {
    name: "CLASSIFICATION"
    data_type: TYPE_FP32
    dims: [ 1000 ]
  },
  {
    name: "SEGMENTATION"
    data_type: TYPE_FP32
    dims: [ 3, 224, 224 ]
  }
]
ensemble_scheduling {
  step [
    {
      model_name: "image_preprocess_model"
      model_version: -1
      input_map {
        key: "RAW_IMAGE"
        value: "IMAGE"
      }
      output_map {
        key: "PREPROCESSED_OUTPUT"
        value: "preprocessed_image"
      }
    },
    {
      model_name: "classification_model"
      model_version: -1
      input_map {
        key: "FORMATTED_IMAGE"
        value: "preprocessed_image"
      }
      output_map {
        key: "CLASSIFICATION_OUTPUT"
        value: "CLASSIFICATION"
      }
    },
    {
      model_name: "segmentation_model"
      model_version: -1
      input_map {
        key: "FORMATTED_IMAGE"
        value: "preprocessed_image"
      }
      output_map {
        key: "SEGMENTATION_OUTPUT"
        value: "SEGMENTATION"
      }
    }
  ]
}
```

ensemble_scheduling部分表示将使用集成调度器，且集成模型由三个不同的模型组成。step部分中的每个元素都指定要使用的模型以及如何将模型的输入和输出映射到调度器识别的张量名称。例如，step中的第一个元素指定要使用image_preprocess_model的最新版本，其输入“RAW_IMAGE”的内容由“IMAGE”张量提供，并且其输出“PREPROCESSED_OUTPUT”将映射到“preprocessed_image”张量以供以后使用。调度器识别的张量名称是ensemble_inputs、ensemble_outputs以及input_map和output_map中的所有值。

组成集成模型的模型也可以启用动态批量处理。由于集成模型只是对组成模型之间的数据进行路由，因此Triton可以在不修改集成模型的配置的情况下将请求发送到集成模型以利用组成模型的动态批量处理。

假设只提供ensemble模型、预处理模型、分类模型和分割模型，客户端应用程序将把它们视为可以独立处理请求的四个不同模型。但是，集成调度器将把集成模型视为以下形式：

![img](http://www.kdocs.cn/api/v3/office/copy/WU8xMDFhcmgrT3lBMVBoUHpIbExwZzlJV1lWeHBaVDl5aERBYy9PNGs1VHNmSGhMTUtoUk1BNjI3ME9zQUtpUWFxdGplWGZ0ODNKUXFSbWZ4NGltTjRhUzUxb0pkeVJ1RXBhdU1oWkxXNmhkSmZBUUVQaVhic1NlVkw3SmhwamlTRld3MUsydUpqK2tPNitkd0x4bnFDNmFsWW9BY2JpblpJdE9SRVJ5dWZKMWhwdDZnVS9vd3BHR1lkdFdaK29xQWdkNWU3aDdPTHdxZW82eTVXbkswNHVCdTdJNDV3Q0dJcW1GckFxZmlvZ2hTdVdpUXNKcnk3TGVWK0xhcXVVdDl5N1BsdTNNWVQ4PQ==/attach/object/XRLSIBIAPY?)



###### 优化设置[Optimization](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/optimization.md#optimization)



- [Model Analyzer](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_analyzer.md)

- - 通过模型分析工具观察不同并发量下的延迟时间以及吞吐量
  - [模型分析工具](https://github.com/triton-inference-server/model_analyzer)  查看资源利用率
  - [模型性能分析](https://github.com/triton-inference-server/client/blob/main/src/c++/perf_analyzer/README.md)  查看模型吞吐量

- [Dynamic Batcher](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/optimization.md#dynamic-batcher) 优化影响

动态批量处理器将各个推断请求组合成一个更大的批量，其执行效率通常远高于单独执行各个请求。要为动态批量处理器启用Triton，请将以下行添加到inception_graphdef的模型配置文件的末尾，然后重新启动Triton。

```
perf_analyzer -m seqcls_model --percentile=95 --concurrency-range=64
```



inception_graphdef示例

```
name: "inception_graphdef"
platform: "tensorflow_graphdef"
max_batch_size: 128
input [
  {
    name: "input"
    data_type: TYPE_FP32
    format: FORMAT_NHWC
    dims: [ 299, 299, 3 ]
  }
]
output [
  {
    name: "InceptionV3/Predictions/Softmax"
    data_type: TYPE_FP32
    dims: [ 1001 ]
    label_filename: "inception_labels.txt"
  }
]
dynamic_batching { }
```

***\*不使用动态批量和使用动态批量的吞吐量和延迟对比\****

| ***\*无动态批量\****                                         | ***\*有动态批量\****                                         |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| $ perf_analyzer -m inception_graphdef --percentile=95 --concurrency-range 1:4...Inferences/Second vs. Client p95 Batch LatencyConcurrency: 1, throughput: 62.6 infer/sec, latency 21371 usecConcurrency: 2, throughput: 73.2 infer/sec, latency 34381 usecConcurrency: 3, throughput: 73.2 infer/sec, latency 50298 usecConcurrency: 4, throughput: 73.4 infer/sec, latency 65569 usec | $ perf_analyzer -m inception_graphdef --percentile=95 --concurrency-range 1:4...Inferences/Second vs. Client p95 Batch LatencyConcurrency: 1, throughput: 66.8 infer/sec, latency 19785 usecConcurrency: 2, throughput: 80.8 infer/sec, latency 30732 usecConcurrency: 3, throughput: 118 infer/sec, latency 32968 usecConcurrency: 4, throughput: 165.2 infer/sec, latency 32974 usec |

- [Model Instances](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/optimization.md#model-instances) 模式实例数优化影响

修改配置文件,将之前的动态批量参数删除,增加下面参数,重启triton

```
instance_group [ { count: 2 }]
```

对比增加实例数,增加实例数+动态批量

| $ perf_analyzer -m inception_graphdef --percentile=95 --concurrency-range 1:4...Inferences/Second vs. Client p95 Batch LatencyConcurrency: 1, throughput: 70.6 infer/sec, latency 19547 usecConcurrency: 2, throughput: 106.6 infer/sec, latency 23532 usecConcurrency: 3, throughput: 110.2 infer/sec, latency 36649 usecConcurrency: 4, throughput: 108.6 infer/sec, latency 43588 usec | $ perf_analyzer -m inception_graphdef --percentile=95 --concurrency-range 16...Inferences/Second vs. Client p95 Batch LatencyConcurrency: 16, throughput: 289.6 infer/sec, latency 59817 usec |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
|                                                              |                                                              |

我们发现，与只使用动态批量处理器和一个实例相比，两个实例并没有显著提高吞吐量，反而增加了延迟。这是因为对于这个模型，仅动态批量处理器就能够充分利用GPU，因此添加额外的模型实例不会提供任何性能优势。一般来说，动态批量处理器和多个实例的益处是具体模型特定的，因此应该使用perf_analyzer进行实验，以确定最适合吞吐量和延迟要求的设置。

- 特定框架优化

- - ONNX模型使用[ TensorRT](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/optimization.md#onnx-with-tensorrt-optimization-ort-trt)优化配置

  - - 配置文件中添加以下位置
    - 对比

```
optimization { execution_accelerators {
  gpu_execution_accelerator : [ {
    name : "tensorrt"
    parameters { key: "precision_mode" value: "FP32" }
    parameters { key: "max_workspace_size_bytes" value: "1073741824" }
    }]
}}
```



| $ perf_analyzer -m densenet_onnx --percentile=95 --concurrency-range 1:4...Inferences/Second vs. Client p95 Batch LatencyConcurrency: 1, 113.2 infer/sec, latency 8939 usecConcurrency: 2, 138.2 infer/sec, latency 14548 usecConcurrency: 3, 137.2 infer/sec, latency 21947 usecConcurrency: 4, 136.8 infer/sec, latency 29661 usec | $ perf_analyzer -m densenet_onnx --percentile=95 --concurrency-range 1:4...Inferences/Second vs. Client p95 Batch LatencyConcurrency: 1, 190.6 infer/sec, latency 5384 usecConcurrency: 2, 273.8 infer/sec, latency 7347 usecConcurrency: 3, 272.2 infer/sec, latency 11046 usecConcurrency: 4, 266.8 infer/sec, latency 15089 usec |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
|                                                              |                                                              |

启用TensorRT优化时，ONNX模型的加载速度可能会明显变慢。在生产环境中，您可以使用模型预热来避免这种模型启动/优化减速

- - ONNX模型使用[OpenVINO](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/optimization.md#onnx-with-openvino-optimization)优化配置

```
optimization { execution_accelerators {
  cpu_execution_accelerator : [ {
    name : "openvino"
  }]
}}
```



```
parameters { key: "gpu_mem_limit" value: { string_value: "2147483648" } }
```



- - tf模型使用[ TensorRT](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/optimization.md#tensorflow-with-tensorrt-optimization-tf-trt) 优化配置

```
optimization { execution_accelerators {
  gpu_execution_accelerator : [ {
    name : "tensorrt"
    parameters { key: "precision_mode" value: "FP16" }}]
}}
```



| $ perf_analyzer -m inception_graphdef --percentile=95 --concurrency-range 1:4...Inferences/Second vs. Client p95 Batch LatencyConcurrency: 1, throughput: 62.6 infer/sec, latency 21371 usecConcurrency: 2, throughput: 73.2 infer/sec, latency 34381 usecConcurrency: 3, throughput: 73.2 infer/sec, latency 50298 usecConcurrency: 4, throughput: 73.4 infer/sec, latency 65569 usec | $ perf_analyzer -m inception_graphdef --percentile=95 --concurrency-range 1:4...Inferences/Second vs. Client p95 Batch LatencyConcurrency: 1, throughput: 140 infer/sec, latency 8987 usecConcurrency: 2, throughput: 195.6 infer/sec, latency 12583 usecConcurrency: 3, throughput: 189 infer/sec, latency 19020 usecConcurrency: 4, throughput: 191.6 infer/sec, latency 24622 usec |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
|                                                              |                                                              |









#### [backend仓库](https://github.com/triton-inference-server/backend#backends) 

> 列举出几个常用的推理框架

##### [Paddle Paddle](https://github.com/triton-inference-server/paddlepaddle_backend)

Widely used Open Source DL Framework

- 官方给的是如下镜像  包含onnx和paddle backend

```
docker pull paddlepaddle/triton_paddle:21.10

docker run -it -d --gpus '"device=0"' --name triton_zyj --shm-size="32g" -p 7001:7001 -p 7002:7002 -p 7003:7003 -p 7004:7004 -v /data/zhangyj/custom_project/model_inference:/model_inference paddlepaddle/triton_paddle:21.10 bash
```



###### 开发环境

- 查看文档使用22.07  ***\*包含paddle backend环境\****

[PaddlePaddle Release Notes](https://docs.nvidia.com/deeplearning/frameworks/paddle-paddle-release-notes/index.html)  [***\*PaddlePaddle Release 22.07\****](https://docs.nvidia.com/deeplearning/frameworks/paddle-paddle-release-notes/rel_22-07.html#rel_22-07)

```
docker pull nvcr.io/nvidia/paddlepaddle:22.07-py3   # 没有tritonserver
docker pull nvcr.io/nvidia/tritonserver:22.07-py3   # 没有paddle backend
docker pull nvcr.io/nvidia/tritonserver:22.07-py3-sdk
```

- The xx.yy-py3 image contains the Triton inference server with support for Tensorflow, PyTorch, TensorRT, ONNX and OpenVINO models.
- The xx.yy-py3-sdk image contains Python and C++ client libraries, client examples, and the Model Analyzer.



```
docker stop triton_zyj
docker rm triton_zyj
# 下面内容暂时没法用
docker run -it -d --gpus '"device=0"' --name triton_zyj_test --shm-size="32g" -p 6001:6001 -p 6002:6002 -p 6003:6003 -p 6004:6004 -v /data/zhangyj/custom_project/model_inference:/model_inference nvcr.io/nvidia/tritonserver:22.07-py3 bash
# 使用官方给的镜像
docker exec -it triton_zyj bash

pip install --upgrade pip -i https://pypi.tuna.tsinghua.edu.cn/simple


pip list

# 不一定安装
pip install paddlepaddle-gpu==2.4.2.post117 -f https://www.paddlepaddle.org.cn/whl/linux/mkl/avx/stable.html  

# 不一定安装
pip install paddlenlp==2.5.2 fast-tokenizer-python -i https://pypi.tuna.tsinghua.edu.cn/simple

pip install fast-tokenizer-python -i https://pypi.tuna.tsinghua.edu.cn/simple

apt-get install language-pack-en-base
~/.bashrc或~/.bash_profile
export LC_ALL=en_US.UTF-8
export LANG=en_US.UTF-8

apt-get install onnx-graphsurgeon
pip install nvidia-pyindex
pip install onnx==1.13.1   -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install nvidia-tensorrt==8.4.1.5 -i https://pypi.tuna.tsinghua.edu.cn/simple  # 最重要的是这个要安装成功,要使用onnx转tensorrt模型功能

# 退出生成新镜像
docker commit -m 'pip paddle' triton_zyj registry.cn-beijing.aliyuncs.com/zhangyj-n/triton-paddle:21.10-py3

docker stop triton_zyj
docker rm triton_zyj

docker run -it -d --gpus '"device=0"' --name triton_zyj --shm-size="32g" -p 7001:7001 -p 7002:7002 -p 7003:7003 -p 7004:7004 -v /data/zhangyj/custom_project/model_inference:/model_inference registry.cn-beijing.aliyuncs.com/zhangyj-n/triton-paddle:21.10-py3 bash  # 还未推向仓库 

# 公司镜像仓库
docker commit -m 'pip paddle' triton_zyj registry.cn-beijing.aliyuncs.com/sg-gie/triton-paddle:21.10-py3
docker run -it -d --gpus '"device=0"' --name triton_zyj --shm-size="32g" -p 7001:7001 -p 7002:7002 -p 7003:7003 -p 7004:7004 -v /data/zhangyj/custom_project/model_inference:/model_inference registry.cn-beijing.aliyuncs.com/sg-gie/triton-paddle:21.10-py3 bash # 在pkm-04上部署
```



***\*tensorrt部署的环境\****

```
docker stop triton_zyj
docker rm triton_zyj
# 下面内容暂时没法用paddle
docker run -it -d --gpus '"device=0"' --name triton_zyj_test --shm-size="32g" -p 6001:6001 -p 6002:6002 -p 6003:6003 -p 6004:6004 -v /data/zhangyj/custom_project/model_inference:/model_inference nvcr.io/nvidia/tritonserver:22.07-py3 bash


docker cp triton_zyj_test:/opt/tritonserver/backends/tensorrt/libtriton_tensorrt.so /data/zhangyj/custom_project/model_inference/
docker cp /data/zhangyj/custom_project/model_inference/libtriton_tensorrt.so triton_zyj:/opt/tritonserver/backends/tensorrt/libtriton_tensorrt.so


tritonserver --model-repository=/model_inference/tensort_backend/models --grpc-port=6003


pip install --upgrade pip -i https://pypi.tuna.tsinghua.edu.cn/simple

pip list

pip install paddlepaddle-gpu==2.4.2.post117 -f https://www.paddlepaddle.org.cn/whl/linux/mkl/avx/stable.html  

pip install paddlenlp==2.5.2 fast-tokenizer-python -i https://pypi.tuna.tsinghua.edu.cn/simple

pip install fast-tokenizer-python -i https://pypi.tuna.tsinghua.edu.cn/simple

pip install nvidia-tensorrt==8.4.1.5 -i https://pypi.tuna.tsinghua.edu.cn/simple  # 最重要的是这个要安装成功,要使用onnx转tensorrt模型功能

docker commit -m 'add nvidia-tensorrt8.4.1.5' triton_zyj_test registry.cn-beijing.aliyuncs.com/sg-gie/tensorrt-paddle:22.07   # 在pkm-04上部署tensorrt模型
docker commit -m 'add nvidia-tensorrt8.4.1.5' triton_zyj_test registry.cn-beijing.aliyuncs.com/zhangyj-n/tensorrt-paddle:22.07
```



- ***\*环境总结\****

- - nvcr.io/nvidia/paddlepaddle:22.07-py3  含tritonserver,paddle_backend,python_backend,onnxruntime_backend

  - - 安装上面各种包
    - 最终生成 [registry.cn-beijing.aliyuncs.com/zhangyj-n/triton-paddle:21.10-py3](https://registry.cn-beijing.aliyuncs.com/zhangyj-n/triton-paddle:21.10-py3)   可以进行onnx转tensorrt

  - nvcr.io/nvidia/tritonserver:22.07-py3 含tritonserver,Python,onnxruntime,torch,tf,tensorrt

  - - 安装上面各种包
    - 最终生成[registry.cn-beijing.aliyuncs.com/zhangyj-n/tensorrt-paddle:22.07](https://registry.cn-beijing.aliyuncs.com/zhangyj-n/tensorrt-paddle:22.07) 可以部署tensorrt模型



- ***\*检查paddle环境\****

```
python3 -c 'import paddle; paddle.utils.run_check()'
Running verify PaddlePaddle program …
W0516 06:36:54.208734   442 device_context.cc:451] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.1, Runtime API Version: 10.2
W1007 08:19:15.730820  5622 gpu_resources.cc:91] device: 0, cuDNN Version: 8.2.
```



###### [模型仓库](https://github.com/triton-inference-server/paddlepaddle_backend/blob/main/docs/zh_CN/model_repository.md)

- 在最顶层`<model-repository-path>`模型仓库目录下，必须有0个或多个`<model-name>`模型名字的子目录。每个`<model-name>`模型名字子目录包含部署模型相应的信息，多个表示模型版本的数字子目录和一个描述模型配置的***config.pbtxt***文件。Paddle模型存在版本号子目录中，必须为`model.pdmodel`文件和`model.pdiparams`文件。

- - Paddle Backend需要的模型必须是2.0版本以上导出的推理模型，模型包含`model.pdmodel`和`model.pdiparams`两个文件放在版本目录中。一个使用Paddle Backend部署的最小模型仓库目录示例:

```
  <model-repository-path>/
    <model-name>/
      config.pbtxt
      1/
        model.pdmodel
        model.pdiparams

  # 真实例子:
  models
  └── ResNet50
      ├── 1
      │   ├── model.pdiparams
      │   └── model.pdmodel
      └── config.pbtxt
```

###### [模型配置](https://github.com/triton-inference-server/paddlepaddle_backend/blob/main/docs/zh_CN/model_configuration.md)



[Paddle TensorRT配置](https://github.com/triton-inference-server/paddlepaddle_backend/blob/main/docs/zh_CN/model_configuration.md#paddle-tensorrt配置)

TensorRT 是一个针对 NVIDIA GPU 及 Jetson 系列硬件的高性能机器学习推理 SDK，可以使得深度学习模型在这些硬件上的部署获得更好的性能。Paddle Inference 以子图方式集成了 TensorRT，将可用 TensorRT 加速的算子组成子图供给 TensorRT，以获取 TensorRT 加速的同时，保留 PaddlePaddle 即训即推的能力。

TensorRT的配置选项需要写在这个配置中: `optimization {execution_accelerators {gpu_execution_accelerator{...}}}`

一共有四个选项:`tensorrt`, `min_shape`, `max_shape`, `opt_shape`.

***\*tensorrt选项\****

在`tensorrt`中能够设置`precision`, `min_graph_size`, `max_batch_size`, `workspace_size`, `enable_tensorrt_oss`, `is_dynamic`. 详细参数解释请看官网文档[Paddle Inference Docs](https://paddle-inference.readthedocs.io/en/latest/api_reference/cxx_api_doc/Config/GPUConfig.html#tensorrt)

| ***\**\*Parameters\*\**\*** | ***\**\*Available options\*\**\***       |
| --------------------------- | ---------------------------------------- |
| precision                   | `"trt_fp32"`, `"trt_fp16"`, `"trt_int8"` |
| min_graph_size              | `"1"` ~ `"2147483647"`                   |
| max_batch_size              | `"1"` ~ `"2147483647"`                   |
| workspace_size              | `"1"` ~ `"2147483647"`                   |
| enable_tensorrt_oss         | `"0"`, `"1"`                             |
| is_dynamic                  | `"0"`, `"1"`                             |

***\*min_shape, max_shape, opt_shape选项\****

当且仅当开启动态shape时(***is_dynamic***为***1***)，每个输入需要设置最大形状(***max_shape***)、最小形状(***min_shape***)和最常见形状(***opt_shape***)。其中字典***parameters***中***key***为输入的名字，***value***为对应输入的最大、最小、最常见shape。

***\*TensorRT动态shape例子\****

```
optimization {
  execution_accelerators {
    gpu_execution_accelerator : [
      {
        name : "tensorrt"
        # 使用TensorRT的FP16推理
        parameters { key: "precision" value: "trt_fp16" }
        # 设置TensorRT的子图最小op数为3
        parameters { key: "min_graph_size" value: "3" }
        parameters { key: "workspace_size" value: "1073741824" }
        # 不使用变长
        parameters { key: "enable_tensorrt_oss" value: "0" }
        # 开启动态shape
        parameters { key: "is_dynamic" value: "1" }
      },
      {
        name : "min_shape"
        parameters { key: "eval_placeholder_0" value: "1" }
        parameters { key: "eval_placeholder_1" value: "1" }
        parameters { key: "eval_placeholder_2" value: "1" }
        parameters { key: "eval_placeholder_3" value: "1 1 1" }
      },
      {
        name : "max_shape"
        parameters { key: "eval_placeholder_0" value: "4096" }
        parameters { key: "eval_placeholder_1" value: "4096" }
        parameters { key: "eval_placeholder_2" value: "129" }
        parameters { key: "eval_placeholder_3" value: "1 128 1" }
      },
      {
        name : "opt_shape"
        parameters { key: "eval_placeholder_0" value: "128" }
        parameters { key: "eval_placeholder_1" value: "128" }
        parameters { key: "eval_placeholder_2" value: "2" }
        parameters { key: "eval_placeholder_3" value: "1 128 1" }
      }
    ]
  }
}
```

###### 模型服务

- 部署模型

```
/model_inference/paddlepaddle_backend/models  # Triton启动需要的模型仓库，包含模型和服务配置文件
seqcls_grpc_client.py     # 分类任务发送pipeline预测请求的脚本
```

- 启动服务端

```
# 启动所有模型
tritonserver --model-repository=/model_inference/paddlepaddle_backend/models --http-port=7001 --grpc-port=7003
# 启动单一模型
/opt/tritonserver/bin/tritonserver --model-repository=/model_inference/paddlepaddle_backend/models --model-control-mode=explicit --load-model=seqcls_model --http-port=7001 --grpc-port=7003
```

- 客户端请求 与Paddle-Server 一致

```
# 在custom_env中安装
pip install grpcio
pip install tritonclient==2.10.0
python seqcls_grpc_client.py # 或者在pycharm中直接执行
```

> 在启动triton镜像时使用的是多卡,则在配置文件里模型也需要多卡,否则报错:uncorrectable ECC error encountered





##### [PyTorch](https://github.com/triton-inference-server/pytorch_backend)

Widely used Open Source DL Framework

##### [TensorFlow](https://github.com/triton-inference-server/tensorflow_backend)

Widely used Open Source DL Framework

##### [TensorRT](https://github.com/triton-inference-server/tensorrt_backend)

NVIDIA [TensorRT](https://developer.nvidia.com/tensorrt) is an inference acceleration SDK that provide a with range of graph optimizations, kernel optimization, use of lower precision, and more.

##### [ONNX](https://github.com/triton-inference-server/onnxruntime_backend)

ONNX Runtime is a cross-platform inference and training machine-learning accelerator.

##### [Python](https://github.com/triton-inference-server/python_backend)

Users can add custom business logic, or any python code/model for serving requests.

##### [Faster Transformer](https://github.com/triton-inference-server/fastertransformer_backend)

NVIDIA [FasterTransformer](https://github.com/NVIDIA/FasterTransformer/) (FT) is a library implementing an accelerated engine for the inference of transformer-based neural networks, with a special emphasis on large models, spanning many GPUs and nodes in a distributed manner.



##### [Backend Shared Library](https://github.com/triton-inference-server/backend#backend-shared-library)



#### client仓库



##### http

##### grpc



#### [教程2-提升资源利用率](https://github.com/triton-inference-server/tutorials/tree/main/Conceptual_Guide/Part_2-improving_resource_utilization)

- 动态批处理和模型并行
- 配置文件添加

```
dynamic_batching { }
```

- Triton可以批量处理传入的请求，而且没有延迟。这种能力使得Triton能够高效地处理大量的请求，但同时也可能因为处理速度过快而导致资源利用率不高。

- - 调度器的作用是收集更多的推理请求，然后由动态批处理器（dynamic batcher）进行处理。"scheduler"可以分配一个有限的延迟时间，这意味着它可以等待一段时间，以收集更多的请求，然后再进行处理。
  - 这种策略的优点是可以提高处理的效率。通过等待一段时间来收集更多的请求，我们可以确保处理器有足够的任务来处理，从而提高其利用率。同时，由于可以控制等待的时间，因此我们也可以在一定程度上控制处理的延迟。
  - 然而，这种策略也有一些缺点。如果等待的时间过长，可能会导致处理速度变慢；如果等待的时间过短，可能会导致处理器无法充分利用资源。因此，用户需要根据实际情况来选择合适的延迟时间。
  - 总的来说，这是一种在处理大量请求时权衡处理速度和资源利用率的策略。

```
dynamic_batching {
    max_queue_delay_microseconds: 100
}
```

- 问题: 不使用动态批量输入,而是一次传入批量数据,怎么设置,性能分析终端命令需要测试



#### [教程3-优化配置](https://github.com/triton-inference-server/tutorials/tree/main/Conceptual_Guide/Part_3-optimizing_triton_configuration)

- 资源占用和推理性能之间的权衡

- 模型请求推理延迟占比分析: 网络延迟,计算时间,由于模型队列中的等待时间导致的延迟

- - 增加网络宽带,加快数据传输
  - 计算时间,第4部分会讲解从模型结构角度减少计算时间
  - 队列延迟,需要分析模型的资源使用率,可以增加模型实例,减少排队等待时间

- model analyzer工具

```
sudo apt-get update && sudo apt-get install python3-pip
sudo apt-get update && sudo apt-get install wkhtmltopdf
pip3 install triton-model-analyzer
```



#### [教程4-推理加速](https://github.com/triton-inference-server/tutorials/tree/main/Conceptual_Guide/Part_4-inference_acceleration)

模型加速是一个复杂且精细的主题。诸如模型图形优化、剪枝、知识蒸馏、量化等技术可行性高度依赖于模型的结构。这些主题每一个都是研究领域，而构建自定义工具需要大量的工程投资。

为了简洁客观起见，本讨论将不涉及生态系统的详尽概述，而是重点介绍在使用Triton推理服务器部署模型时建议使用的工具和功能。

![img](http://www.kdocs.cn/api/v3/office/copy/WU8xMDFhcmgrT3lBMVBoUHpIbExwZzlJV1lWeHBaVDl5aERBYy9PNGs1VHNmSGhMTUtoUk1BNjI3ME9zQUtpUWFxdGplWGZ0ODNKUXFSbWZ4NGltTjRhUzUxb0pkeVJ1RXBhdU1oWkxXNmhkSmZBUUVQaVhic1NlVkw3SmhwamlTRld3MUsydUpqK2tPNitkd0x4bnFDNmFsWW9BY2JpblpJdE9SRVJ5dWZKMWhwdDZnVS9vd3BHR1lkdFdaK29xQWdkNWU3aDdPTHdxZW82eTVXbkswNHVCdTdJNDV3Q0dJcW1GckFxZmlvZ2hTdVdpUXNKcnk3TGVWK0xhcXVVdDl5N1BsdTNNWVQ4PQ==/attach/object/UZ2CKBIAUY?)

- [使用TensorRT](https://github.com/triton-inference-server/tutorials/tree/main/Conceptual_Guide/Part_4-inference_acceleration#using-tensorrt-directly)

- - 将onnx转为tensorrt模型

```
trtexec --onnx=model.onnx \
        --saveEngine=model.plan \
        --explicitBatch
```

Once converted, place the model in the `model.plan` in the model repository (as described in part 1) and use `tensorrt` as the `backend` in the `config.pbtxt`

- - 直接使用tensorRT后端

  - [Using TensorRT's integration with ONNX RunTime](https://github.com/triton-inference-server/tutorials/tree/main/Conceptual_Guide/Part_4-inference_acceleration#using-tensorrts-integration-with-onnx-runtime)

  - - 有三种选择来加速ONNX运行时：使用TensorRT和CUDA执行提供程序进行GPU加速，使用OpenVINO（在后续部分讨论）进行CPU加速。通常，TensorRT会提供比CUDA执行提供者更好的优化，但这取决于模型的确切结构，更具体地说，取决于正在加速的网络中使用的操作。如果所有操作都受支持，则转换为TensorRT将产生更好的性能。当选择TensorRT作为加速器时，所有支持的子图都由TensorRT加速，其余图形在CUDA执行提供程序上运行。用户可以通过向配置文件中添加以下内容来实现这一点。

```
optimization {
  execution_accelerators {
    gpu_execution_accelerator : [ {
      name : "tensorrt"
      parameters { key: "precision_mode" value: "FP32" }
      parameters { key: "max_workspace_size_bytes" value: "1073741824" }
    }]
  }
}
```

#### [教程5-模型集成](https://github.com/triton-inference-server/tutorials/tree/main/Conceptual_Guide/Part_5-Model_Ensembles)





#### [教程6-构件pipelines](https://github.com/triton-inference-server/tutorials/tree/main/Conceptual_Guide/Part_6-building_complex_pipelines)



#### Paddle-Onnx-Server示例

##### 开发环境

- - 客户端请求在custom_env环境下执行
  - 服务端部署在triton_zyj环境下执行
  - 代码开发路径 custom_env 下/workspace/custom_project/model_inference,triton_zyj外挂此数据卷
  - ***\*不含paddle backend\****
  - ***\*非paddle自己的推理框架\****
  - 使用triton启动 onnx

```
docker pull nvcr.io/nvidia/tritonserver:21.10-py3
docker run  -it -d --gpus all --name triton_zyj --shm-size="16g" -v /data/zhangyj/custom_project/model_inference:/model_inference nvcr.io/nvidia/tritonserver:21.10-py3 bash
docker exec -it triton_zyj bash

pip install paddlepaddle-gpu==2.4.2.post117 -f https://www.paddlepaddle.org.cn/whl/linux/mkl/avx/stable.html

pip install paddlenlp==2.5.2 fast-tokenizer-python -i https://pypi.tuna.tsinghua.edu.cn/simple

# 退出生成新镜像
docker commit -m 'pip paddle' triton_zyj registry.cn-beijing.aliyuncs.com/zhangyj-n/triton:21.10-py3

docker stop triton_zyj
docker rm triton_zyj

docker run -it -d --gpus '"device=0"' --name triton_zyj_test --shm-size="16g" -p 6001:6001 -p 6002:6002 -p 6003:6003 -p 6004:6004 -v /data/zhangyj/custom_project/model_inference:/model_inference registry.cn-beijing.aliyuncs.com/zhangyj-n/triton:21.10-py3 bash
```



##### 模型转换

> ***[个人github]***(https://github.com/kg-nlp/model_transformer)

> 参考资料 [向量模型训练](https://kg-nlp.github.io/Algorithm-Project-Manual/框架环境/模型转换.html)

- 配置文件

[Triton Server Model Configuration](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md)

详细配置见Triton-github源文件分析

```
|-- README.md
`-- paddle_triton_serving
    |-- README.md
    |-- models
    |   |-- seqcls
    |   |   |-- 1
    |   |   `-- config.pbtxt
    |   |-- seqcls_model
    |   |   |-- 1
    |   |   |   `-- model.onnx
    |   |   `-- config.pbtxt
    |   |-- seqcls_postprocess
    |   |   |-- 1
    |   |   |   `-- model.py
    |   |   `-- config.pbtxt
    |   `-- tokenizer
    |       |-- 1
    |       |   `-- model.py
    |       `-- config.pbtxt
    `-- seqcls_grpc_client.py
```

- 部署模型

```
/model_inference/paddle_triton_serving/models  # Triton启动需要的模型仓库，包含模型和服务配置文件
seqcls_grpc_client.py     # 分类任务发送pipeline预测请求的脚本
```

##### 启动服务端

```
# 启动所有模型
tritonserver --model-repository=/model_inference/paddle_triton_serving/models --http-port=7001 --grpc-port=7003

tritonserver --model-repository=/model_inference/paddle_triton_serving/models --http-port=6001 --grpc-port=6003

# 启动单一模型
tritonserver --model-repository=/model_inference/paddle_triton_serving/models --model-control-mode=explicit --load-model=seqcls_model --http-port=7001 --grpc-port=7003
```

- 客户端请求

```
# 在custom_env中安装
pip install grpcio
pip install tritonclient==2.10.0
python seqcls_grpc_client.py # 或者在pycharm中直接执行
```

> 在启动triton镜像时使用的是多卡,则在配置文件里模型也需要多卡,否则报错:uncorrectable ECC error encountered



## 对比总结

### 测试

- 规则

- - 对比 paddle单个模型服务和triton单个模型服务的推理时间
  - 对比tf单个模型服务和triton单个模型服务的推理时间
  - 对比模型推理pipeline的推理时间
  - 资源约束为单卡,单实例
  - 需要单独看模型预测的推理时间,分词处理时间,后处理时间
  - 输入批量数据:输入64条数据,设置max_bachsize=64
  - 输入数据格式为文本列表,输出数据格式为已映射标签后的文本列表
  - 比较策略:***\*单进程请求\****
  - 列举不同配置文件的测试效果

- 对比的策略有三种:PaddleServing,基于paddle backend的triton server,基于onnx的triton server

- - paddle serving已经引入了triton 加速方法,但在配置文件里目前没有使用

- 评价结果以线上全流程测试为主(单指模型pipelines)



截止 2023年10月11日 星期三,经过优化有的算法部分推理 由7.765512228012085s 降至 4.536190509796143s 提速41%



### 优化方向

- 统计前处理,模型推理,后处理时间占比

- 模型推理优化

- - 增加资源
  - 推理优化设置

- 前后处理优化

- - 这部分时间占比也很大

- 先把模型优化后进行部署(T4)再优化工程代码

### 优化方法

- 前处理使用***\**\*fast-tokenizer-python\*\**\*** ***\**\*,速度提升会非常明显\*\**\***
- 后处理使用numpy计算阈值,对于多分类多标签,分类标签数量越多,这种优化提升越明显
- 前处理和后处理 另起服务,模型预测使用TritonServer,这样比在triton中直接部署前后处理要快10%-20%
- 安培架构的 GPU，推荐使用 CUDA11 以上。非安培架构的 GPU，推荐使用 CUDA10.2，性能更优
- 采用静态图预测,比onnx节省显存,预测更快

## 参考链接

- Paddle-Series

- - [Paddle-基于Triton Inference Server的服务化部署指南](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/applications/text_classification/hierarchical/deploy/triton_serving)
  - [Paddle Inference Demos](https://github.com/PaddlePaddle/Paddle-Inference-Demo) & [Paddle Inference 简介](https://www.paddlepaddle.org.cn/inference/master/guides/index_guides.html) & [docs.io](https://paddle-inference.readthedocs.io/en/latest/api_reference/python_api_doc/python_api_index.html)
  - [paddle2.4.2-GPU相关版本](https://www.paddlepaddle.org.cn/documentation/docs/zh/2.4/install/docker/linux-docker.html)

- [NVIDIA TensorRT official website](https://developer.nvidia.cn/tensorrt)

- - [Speeding Up Deep Learning Inference Using TensorRT](https://developer.nvidia.com/blog/speeding-up-deep-learning-inference-using-tensorrt/) & [NVIDIA 深度学习 TensorRT 文档：简介与能力](https://zhuanlan.zhihu.com/p/637269673)
  - [NVIDIA Deep Learning TensorRT Documentation](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#python_example_unsupported)  
  - [Sample Support Guide](https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html)[docs.io](https://paddle-inference.readthedocs.io/en/latest/api_reference/python_api_doc/python_api_index.html)
  - [github](https://github.com/NVIDIA/TensorRT)

- [Triton Inference Server](https://github.com/triton-inference-server/server#triton-inference-server)

- - [Triton Inference Server -Nvidia docker](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver)
  - [github](https://github.com/triton-inference-server/server)
  - [triton server 中文博客系列教程](https://blog.csdn.net/searobbers_duck?type=blog)
  - [Nvidia DeepStream和Triton Server学习经验分享](https://zhuanlan.zhihu.com/p/640556970)
  - [Nvidia Triton 使用教程](https://maple.link/2021/06/10/Nvidia Triton Server的使用/)



[史上最全面的AI推理框架对比--OpenVINO、TensorRT、Mediapipe](https://zhuanlan.zhihu.com/p/344442534)

[如何优化深度学习模型以提升推理速度](https://zhuanlan.zhihu.com/p/417536087)

[DeepLearningSystem](https://github.com/chenzomi12/DeepLearningSystem)

[使用TensorRT遇到过什么坑呢？](https://www.zhihu.com/question/4592524)

[TensorRT_Tutorial](https://github.com/LitLeo/TensorRT_Tutorial)



- 其他概念

- - P90 Latency 是一个在性能测试和网络工程中使用的术语，用来描述在所有测量数据中，延迟时间（或响应时间）达到或者超过某个特定阈值（这里是90%）的请求或数据包的数量。换句话说，P90 Latency 是90%的网络请求或数据包的延迟时间。这个概念可以用来衡量网络性能的整体水平，以及特定应用的性能表现。
  - ***\*算子融合\**** [***\*https://zhuanlan.zhihu.com/p/581755093\****](https://zhuanlan.zhihu.com/p/581755093)
  - 内存显存优化
